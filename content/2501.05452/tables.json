[{"content": "| Model | VWTQ | VWTQ_syn | VTabFact | CharXiv | Horizontal Bar | Vertical Bar |\n|---|---|---|---|---|---|---|\n| **Table** |  |  |  |  |  |  |\n| **Chart** |  |  |  |  |  |  |\n| *Prior Multimodal LMs* |  |  |  |  |  |  |\n| LLaVA-NeXT-34B [20] | 36.4 | 38.0 | 71.2 | 18.9 | 23.4 | 12.6 |\n| Phi 3 vision [1] | 44.7 | 53.2 | 74.4 | 16.2 | 60.8 | 66.5 |\n| Gemini-Pro 1.5 [33] | 38.5 | 43.2 | 75.6 | 38.3 | 57.2 | 66.0 |\n| VisProg [10] | 53.2 | 62.0 | 76.4 | 46.8 | 69.8 | 68.6 |\n| *Latest multimodal LLMs + ReFocus* |  |  |  |  |  |  |\n| gpt-4o-2024-05-13 [26] | 66.5 | 73.2 | 89.6 | 49.0 | 78.2 | 76.2 |\n| + ReFocus | 76.9 | 79.6 | 89.6 | **57.3** | **85.4** | 81.0 |\n|  | +10.4 | +3.4 | +0.0 | +8.3 | +7.2 | +4.8 |\n| gpt-4o-2024-08-06 [26] | 66.4 | 70.4 | 90.0 | 48.9 | 75.2 | 74.9 |\n| + ReFocus | **77.2** | **82.8** | **90.8** | 46.2 | 82.0 | **81.2** |\n|  | +9.8 | +12.4 | +0.8 | -2.7 | +5.0 | +4.2 |", "caption": "Table 1: ReFocus yields consistent performance gains across all tasks and outperforms all baselines. Notice that the GPT baselines here are also in a conversational format but without editing abilities. For fair comparison, we modify the original Visprog\u00a0[10] framework by replacing the LM and VQA components with the latest GPT-4o model.", "description": "This table presents a comprehensive comparison of various multimodal large language models (LLMs) on several structured image understanding tasks, including those involving tables and charts.  It showcases the performance gains achieved by integrating the REFOCUS framework, which enhances the LLMs by incorporating visual editing.  The table compares REFOCUS-enhanced models against several baseline models, both prior multimodal LLMs and the latest GPT-4 model without visual editing.  For a fair comparison, the original VisProg framework (a visual programming approach) was adapted using the most up-to-date GPT-4 model to replace its original language model and visual question answering components. The results demonstrate consistent performance improvements across all tasks when using the REFOCUS framework.", "section": "4. Results"}, {"content": "| Model | VWTQ | VWTQ_syn | VTabFact | CharXiv | Horizontal Bar | Vertical Bar |\n|---|---|---|---|---|---|---|\n| Multimodal LLMs with original visual inputs |\n| LLaVA-NeXT-7B [20] | 21.7 | 24.0 | 56.8 | 16.1 | 8.1 | 7.3 |\n| LLaVA-NeXT-13B [20] | 25.6 | 30.4 | 62.4 | 18.9 | 13.5 | 13.1 |\n| LLaVA-NeXT-34B [20] | 36.4 | 38.0 | 71.2 | 18.9 | 23.4 | 12.6 |\n| Phi 3 vision [1] | 44.7 | 53.2 | **74.4** | 16.2 | **60.8** | **66.5** |\n| Multimodal LLMs with ReFocus edited visual inputs |\n| LLaVA-NeXT-7B + Oracle ReFocus | 25.7 | 26.8 | 55.2 | 15.4 | 6.8 | 8.4 |\n| LLaVA-NeXT-13B + Oracle ReFocus | 30.3 | 31.6 | 61.2 | 17.5 | 15.5 | 13.1 |\n| LLaVA-NeXT-34B + Oracle ReFocus | 39.1 | 41.2 | 68.0 | **21.0** | 26.1 | 15.2 |\n| Phi 3 vision + Oracle ReFocus | **48.3** | **56.4** | 72.8 | 17.6 | 60.4 | **66.5** |", "caption": "Table 2: Open-source models\u2019 performance upon the original visual input versus GPT-4o + ReFocus edited images as visual input (referred by + oracle ReFocus).\nNotice that + oracle ReFocus uses the visual artifact generated in the last action of GPT-4o + ReFocus as inputs.", "description": "This table compares the performance of several open-source multimodal large language models (LLMs) on structured image understanding tasks.  It contrasts their accuracy when processing original images versus when using images that have been edited by the ReFocus method.  Specifically, the '+ oracle ReFocus' results show the LLM's performance using only the final, edited image produced by ReFocus, demonstrating the impact of ReFocus's visual editing on the model's ability to understand structured images.", "section": "4. Results"}, {"content": "| Model | VWTQ | VWTQ_syn | VTabFact | CharXiv | Horizontal Bar | Vertical Bar |\n|---|---|---|---|---|---|---|\n| **Table** |  |  |  |  |  |  |\n| GPT-4o Model |  |  |  |  |  |  |\n| Text input | 68.1 | 69.6 | 80.0 | \\ | 66.2 | 74.9 |\n| Figure Input | 61.0 | 68.4 | 88.4 | 47.9 | 75.2 | 74.9 |\n| Text + Figure Input | 67.5 | 72.8 | 91.6 | \\ | 75.7 | 81.7 |\n| GPT-4o + ReFocus |  |  |  |  |  |  |\n| Figure Input | 77.2 | 82.8 | 90.8 | 57.3 | 85.4 | 81.2 |\n|  | +16.2 | +14.4 | +2.4 | +10.6 | +9.7 | +0.5 |", "caption": "Table 3: ReFocus empowers GPT-4o to achieve the performance as if given gold text input. The text input are mainly csv tables, as detailed in Section\u00a03.1. Notice that Table\u00a01 reports the conversational performance without visual editing, whereas the performance discussed here is based on direct question answering, leading to minor differences.", "description": "This table compares the performance of GPT-4o with and without ReFocus on various structured image understanding tasks.  The \"gold text input\" refers to providing the model with the text version of the tables instead of the images. The key finding is that ReFocus improves GPT-4o's performance to a level comparable to using the text data directly. The table highlights the gains from visual editing, especially considering that Table 1 shows conversational performance without visual edits, and this table focuses on direct question answering, leading to slight discrepancies in scores.", "section": "4. Results"}, {"content": "| Dataset | Original | Mask out | Draw Box | Highlight |\n|---|---|---|---|---|\n| VWTQ | 66.4 | 77.2 | **77.6** | 74.8 |\n| VWTQ_syn | 70.4 | **82.4** | 78.8 | 80.8 |", "caption": "Table 4: Analysis on how different editing tool can affect model performance. We control the tool type provided by ReFocus and experiment on the VWTQ and VWTQ_syn datasets.", "description": "This table presents the results of an ablation study to analyze how different visual editing tools used in the REFOCUS framework impact model performance.  The study controls for the type of visual editing tool (mask out, draw box, highlight) while keeping the overall methodology consistent.  The evaluation metrics are applied to two datasets, VWTQ and VWTQ_syn,  allowing for comparison of the effectiveness across different data and editing techniques.", "section": "4.3. Analyses"}, {"content": "|           | Horizontal Bar | Vertical Bar | Avg. |\n|---|---|---|---|\n| *QA Prompting* |           |           |           |\n| Phi-3.5-vision [12] | 60.1 | 63.1 | 61.5 |\n| SFT w/ QA Data | 60.1 | 65.5 | 62.6 |\n| *Visual CoT Prompting* |           |           |           |\n| Phi-3.5-vision [12] | 69.4 | 66.8 | 68.2 |\n| SFT w/ QA Data | 60.6 | 66.8 | 63.4 |\n| SFT w/ ReFocus CoT | 67.1 | 70.7 | 68.8 |\n| SFT w/ ReFocus VCoT | **71.0** | **72.2** | **71.4** |", "caption": "Table 5: SFT accuracy results. The difference between ReFocus VCoT data and CoT data is that VCoT contains refocus area bounding box coordinates whereas CoT does not.\nAll trainings select the best performing hyper-parameters on the same set of training data.", "description": "This table presents the results of supervised fine-tuning (SFT) experiments on the Phi-3.5-vision model.  The model was fine-tuned using three different datasets: standard Question Answering (QA) pairs, Chain-of-Thought (CoT) data, and ReFocus Visual Chain-of-Thought (VCoT) data. The ReFocus VCoT data includes bounding box coordinates specifying the areas of focus within the images, which is the key difference from the other two datasets. The table shows the accuracy achieved on Horizontal and Vertical Bar chart tasks for each dataset and prompting method (QA prompting and Visual CoT prompting).  The results highlight the impact of incorporating visual reasoning information (bounding boxes) into the training data on model performance.", "section": "5. Finetune with REFOCUS data"}, {"content": "|               | Horizontal Bar | Vertical Bar | Total |\n| :------------ | :---------------: | :------------: | :----: |\n| QA Data [24] |      4,990      |     10,069     | 15,059 |\n| ReFocus Data  |      4,722      |      9,622     | 14,344 |\n| w/ Editing   |      4,220      |      8,599     | 12,819 |", "caption": "Table 6: Detailed statistics about ReFocus Data. We count the total number of our training cases, and the ones with visual editing.", "description": "This table presents a statistical summary of the dataset used for training in the REFOCUS model. It shows the total number of training samples in the ChartQA dataset, the number of samples that involved visual editing, and the breakdown of these numbers across different chart types (horizontal bar, vertical bar, etc.). This information provides insights into the composition and characteristics of the dataset used to train the model.", "section": "5. Finetune with REFOCUS data"}, {"content": "| Header | w/ default QA Data | w/ ReFocus VCoT | w/ ReFocus CoT |\n|---|---|---|---|\n| data type | bf16 | bf16 | bf16 |\n| batch size | 64 | 64 | 64 |\n| learning rate | 5e-07 | 1e-06 | 5e-06 |\n| epoch number | 2 | 2 | 2 |\n| include edited image in input | No | No | Yes |", "caption": "Table 7: Hyper-parameter settings for our best fine-tuned models.", "description": "This table presents the hyperparameter settings used to achieve the best performance in the fine-tuning experiments of the REFOCUS model.  It lists the data type, batch size, learning rate, number of epochs, and whether or not edited images were included as input for three different fine-tuning scenarios: with default QA data, with REFOCUS VCoT (visual chain-of-thought) data, and with REFOCUS CoT data.", "section": "4.1 Baselines and Setups"}]