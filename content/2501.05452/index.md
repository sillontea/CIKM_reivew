---
title: "ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding"
summary: "REFOCUS: boosting multimodal LLMs' structured image understanding via visual chain-of-thought!"
categories: ["AI Generated", ]
tags: ["Computer Vision", "Visual Question Answering", "üè¢ University of Pennsylvania",]
showSummary: true
date: 2025-01-09
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2501.05452 {{< /keyword >}}
{{< keyword icon="writer" >}} Xingyu Fu et el. {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2501.05452" target="_self" >}}
‚Üó arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2501.05452" target="_self" >}}
‚Üó Hugging Face
{{< /button >}}
{{< button href="https://paperswithcode.com/paper/refocus-visual-editing-as-a-chain-of-thought" target="_self" >}}
‚Üó Papers with Code
{{< /button >}}




### TL;DR


{{< lead >}}

Current multimodal LLMs struggle with structured image understanding because they lack the ability to strategically refocus on different parts of an image.  This requires multi-hop visual reasoning, which is a significant challenge.  Existing methods often extract image information into text first, limiting their ability to refine understanding iteratively. 

REFOCUS addresses this by equipping multimodal LLMs with the ability to perform visual editing (using Python code) on the input image, generating 'visual thoughts'. This iterative process allows the model to strategically focus on relevant information, enhancing its reasoning capabilities. Experiments show significant performance gains across various structured image understanding tasks, demonstrating the effectiveness of REFOCUS and the value of visual chain-of-thought data for training.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} REFOCUS enhances multimodal LLMs by integrating visual reasoning as an intermediate step, improving performance on structured image understanding. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} REFOCUS uses simple visual edits (drawing boxes, highlighting, masking) generated by the LLM to refine its focus and improve accuracy. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} A new 14k training dataset created with REFOCUS outperforms standard VQA data, highlighting the effectiveness of visual chain-of-thought supervision. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
This paper is important because it introduces a novel framework, **REFOCUS**, that significantly improves the visual reasoning capabilities of multimodal LLMs on structured image understanding tasks.  It also presents a novel training dataset collected using REFOCUS, demonstrating the benefits of visual chain-of-thought supervision. This work opens new avenues for research in improving visual reasoning abilities of LLMs and developing more effective multimodal learning strategies.

------
#### Visual Insights



![](https://arxiv.org/html/2501.05452/x2.png)

> üîº This figure illustrates the ReFocus framework. ReFocus enhances large language models (LLMs) by incorporating visual reasoning steps.  It uses Python code to perform image editing, refining attention to key visual elements.  The process is iterative; the LLM generates code for a visual editing operation, the editing is performed on the image, and the modified image is fed back into the LLM. This continues until an answer is produced. The example shown depicts a TableVQA task where irrelevant columns are masked, and relevant rows are highlighted with a bounding box.
> <details>
> <summary>read the caption</summary>
> Figure 1: Overview of ReFocus. ReFocus performs visual chain of thought via input-image editing on an example data from TableVQA¬†[16]. Given an image and question pair, ReFocus equips GPT-4 with editing tools (details in ¬ß3), and GPT-4 generates pseudo code if an edit action is needed. ReFocus then executes the editing actions, and feeds GPT-4 with the new image until an answer is reached. In the above example, mask_column and draw_row are performed.
> </details>





{{< table-caption >}}
| Model | VWTQ | VWTQ_syn | VTabFact | CharXiv | Horizontal Bar | Vertical Bar |
|---|---|---|---|---|---|---|
| **Table** |  |  |  |  |  |  |
| **Chart** |  |  |  |  |  |  |
| *Prior Multimodal LMs* |  |  |  |  |  |  |
| LLaVA-NeXT-34B [20] | 36.4 | 38.0 | 71.2 | 18.9 | 23.4 | 12.6 |
| Phi 3 vision [1] | 44.7 | 53.2 | 74.4 | 16.2 | 60.8 | 66.5 |
| Gemini-Pro 1.5 [33] | 38.5 | 43.2 | 75.6 | 38.3 | 57.2 | 66.0 |
| VisProg [10] | 53.2 | 62.0 | 76.4 | 46.8 | 69.8 | 68.6 |
| *Latest multimodal LLMs + ReFocus* |  |  |  |  |  |  |
| gpt-4o-2024-05-13 [26] | 66.5 | 73.2 | 89.6 | 49.0 | 78.2 | 76.2 |
| + ReFocus | 76.9 | 79.6 | 89.6 | **57.3** | **85.4** | 81.0 |
|  | +10.4 | +3.4 | +0.0 | +8.3 | +7.2 | +4.8 |
| gpt-4o-2024-08-06 [26] | 66.4 | 70.4 | 90.0 | 48.9 | 75.2 | 74.9 |
| + ReFocus | **77.2** | **82.8** | **90.8** | 46.2 | 82.0 | **81.2** |
|  | +9.8 | +12.4 | +0.8 | -2.7 | +5.0 | +4.2 |{{< /table-caption >}}

> üîº This table presents a comprehensive comparison of various multimodal large language models (LLMs) on several structured image understanding tasks, including those involving tables and charts.  It showcases the performance gains achieved by integrating the REFOCUS framework, which enhances the LLMs by incorporating visual editing.  The table compares REFOCUS-enhanced models against several baseline models, both prior multimodal LLMs and the latest GPT-4 model without visual editing.  For a fair comparison, the original VisProg framework (a visual programming approach) was adapted using the most up-to-date GPT-4 model to replace its original language model and visual question answering components. The results demonstrate consistent performance improvements across all tasks when using the REFOCUS framework.
> <details>
> <summary>read the caption</summary>
> Table 1: ReFocus yields consistent performance gains across all tasks and outperforms all baselines. Notice that the GPT baselines here are also in a conversational format but without editing abilities. For fair comparison, we modify the original Visprog¬†[10] framework by replacing the LM and VQA components with the latest GPT-4o model.
> </details>





### In-depth insights


#### Visual Chain of Thought
The concept of "Visual Chain of Thought" presents a powerful advancement in multimodal reasoning.  It extends the successful "Chain of Thought" prompting paradigm by integrating visual processing and manipulation as integral steps in the reasoning process.  Instead of relying solely on textual intermediate steps, **Visual Chain of Thought leverages visual editing operations** ‚Äì such as drawing boxes, highlighting regions, or masking irrelevant areas ‚Äì to guide the model's attention and refine its understanding of the visual input. This iterative process of visual analysis and manipulation, coupled with textual reasoning, allows the model to tackle complex visual tasks that require multi-step reasoning and selective attention.  **Key benefits include improved visual grounding, reduced hallucinations, and enhanced model performance** on tasks involving structured images like tables and charts.  The introduction of visual edits as intermediate steps provides a more interpretable and insightful reasoning pathway, offering valuable insights into how the model arrives at its conclusions.  **Furthermore, training models on Visual Chain of Thought data appears to yield superior performance** compared to traditional training approaches, suggesting a novel and effective method for improving multimodal reasoning abilities.

#### Visual Editing Effects
The effects of visual editing on image understanding models are multifaceted and significant.  **REFOCUS leverages Python code to perform edits like drawing boxes, highlighting, and masking image regions**. This approach allows the model to selectively focus attention on relevant parts of the image, filtering out distracting elements.  The impact on model performance is substantial, with significant improvements seen across various structured image understanding tasks involving tables and charts, showcasing **enhanced visual reasoning capabilities**.  Interestingly, these improvements aren't solely due to additional information provided by the edits, but rather from improved attention mechanisms and a reduction in model hallucinations.  This points to **the crucial role of visual attention guidance in effective reasoning** in multimodal models.  The effectiveness also depends on the type of edit employed, with some edits contributing more than others to improved performance, highlighting the importance of thoughtful, task-appropriate edit selection.  Further research on this method may reveal insights into optimizing visual reasoning strategies for multimodal AI.

#### REFOCUS Dataset
A hypothetical REFOCUS dataset would be crucial for training and evaluating the proposed visual editing framework.  Its design would need careful consideration.  **Data diversity** is paramount; it should include a wide variety of structured images (tables, charts, forms, etc.) with diverse layouts, styles, and levels of complexity.  **Annotation quality** is also vital; each image would require detailed annotations indicating the optimal sequence of visual edits, including their type (masking, highlighting, drawing boxes), target regions, and the reasoning behind each edit.  **Task variety** is important to make the dataset broadly applicable; it could include tasks like visual question answering, information extraction, and table interpretation.  **A robust evaluation metric** that captures the model's ability to perform effective visual reasoning and achieve high accuracy would be necessary.  Finally, **dataset size** should be substantial enough to support the training of large, powerful multimodal models. The dataset's value would further depend on factors like its accessibility and documentation, encouraging reproducible research and fostering community contribution.

#### Multimodal LLM gains
Analyzing potential gains from multimodal LLMs requires a nuanced approach.  **Improved visual reasoning** is a key area; these models struggle with multi-hop reasoning and selective attention in complex images like tables and charts.  **REFOCUS** addresses this by enabling LLMs to generate visual thoughts via image editing, iteratively refining their focus and improving performance.  **Data augmentation** is another factor; training sets for structured image understanding are smaller and less diverse than natural image datasets.  Using REFOCUS to generate chain-of-thought data with intermediate information creates a richer training signal.  **The simple approach of visual editing** in REFOCUS surprisingly yields significant performance gains, highlighting the potential of combining LLMs with simple yet strategic visual processing.   However, **careful consideration** of the limitations is important;  generalizing to unseen data and chart types remains a challenge, while the computational cost of visual processing needs to be balanced with potential gains. Further research should explore optimizing the visual editing process and expanding the range of applicable visual tasks.

#### Future Work
Future research directions stemming from this paper on REFOCUS could explore several promising avenues.  **Expanding the scope of visual editing tools** beyond the current set would significantly enhance the model's capabilities. Incorporating more sophisticated tools, such as those enabling complex image manipulations or semantic understanding, could lead to more nuanced visual reasoning. Another crucial direction involves **improving the efficiency of the visual reasoning process**. Currently, the iterative editing process can be time-consuming.  Investigating methods to optimize this workflow, perhaps by leveraging more advanced planning or reinforcement learning techniques, is essential.  Furthermore, **exploring the generalizability of REFOCUS** to different types of multimodal LLMs and diverse visual data modalities is vital.  **Benchmarking REFOCUS** against a wider range of state-of-the-art visual reasoning models on a variety of challenging datasets is necessary to comprehensively evaluate its effectiveness. Finally, a deeper investigation into the **theoretical underpinnings of visual chain-of-thought** reasoning would be highly beneficial, potentially uncovering new insights into how LLMs process visual information. This could inform the design of more effective and robust visual reasoning methods for future multimodal AI systems.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2501.05452/x3.png)

> üîº This figure demonstrates how ReFocus improves visual grounding in a chart question answering (ChartQA) task. The left panel shows an original horizontal bar chart, where GPT-4 incorrectly identifies the bars to answer the question. ReFocus modifies the image by visually editing it. This allows GPT-4 to correctly interpret the chart and produce the correct answer (right panel). The improvement highlights how ReFocus enhances the reasoning process.
> <details>
> <summary>read the caption</summary>
> Figure 2: Example of how ReFocus + GPT-4o solves previously unsolvable problem in ChartQA dataset¬†[24] through improved visual grounding. Given the original horizontal bar image (left), GPT-4o grounds to the wrong bars and thus gets the wrong answer. ReFocus eliminates such possibility through editing, guiding the model to the correct answer (right).
> </details>



![](https://arxiv.org/html/2501.05452/x4.png)

> üîº This figure demonstrates how REFOCUS, a framework that incorporates visual editing into the reasoning process of large language models (LLMs), improves the ability of GPT-4 to solve complex problems. The example shown involves the ChartXiv dataset, specifically a chart with four subplots. REFOCUS edits the original image by masking out three irrelevant subplots, thereby enabling GPT-4 to focus solely on the relevant subplot and reach the correct answer, overcoming limitations of standard LLMs that struggle with such multi-hop visual reasoning problems.
> <details>
> <summary>read the caption</summary>
> Figure 3: ReFocus equips GPT-4 with selective attention. Above is an example of how ReFocus + GPT-4o solves previously unsolvable problem in ChartXiv dataset¬†[34]. Specifically, ReFocus edits upon the original image by masking out all irrelevant information ‚Äì the other three subplots that could be distracting. As a result, GPT-4o is able to conduct better reasoning with the edited image, and reach the correct answer.
> </details>



![](https://arxiv.org/html/2501.05452/x5.png)

> üîº Figure 4 demonstrates how REFOCUS improves GPT-4's performance on a visual question answering task involving vertical bar charts from the ChartQA dataset.  The left panel shows GPT-4's original response to the question 'How many years have value less than 10%' applied to a chart displaying year-over-year percentage changes. GPT-4 incorrectly identifies four years. The right panel presents the improved response after employing REFOCUS. REFOCUS guides GPT-4 to focus on the relevant parts of the chart, leading to the correct answer of five years.
> <details>
> <summary>read the caption</summary>
> Figure 4: ReFocus unleashes better visual grounding and counting abilities for GPT-4 as in ChartQA¬†[24] Vertical Bar problems.
> </details>



![](https://arxiv.org/html/2501.05452/x6.png)

> üîº The figure showcases how REFOCUS enhances the performance of GPT-4 in Optical Character Recognition (OCR) tasks within the context of structured image understanding.  Specifically, it demonstrates the use of the 'highlight_column' visual editing function within the REFOCUS framework. By highlighting a relevant column in a table image, REFOCUS guides GPT-4 to focus its attention on the crucial region, thereby improving the accuracy of character recognition and reducing errors.  This example is taken from the TableVQA dataset.
> <details>
> <summary>read the caption</summary>
> Figure 5: ReFocus unleashes better OCR for GPT-4. In this example from TableVQA¬†[16], ReFocus + GPT-4 conducts the edit action highlight_column. With this simple action, GPT-4 can focus more on the important subarea, and recognize the characters better.
> </details>



![](https://arxiv.org/html/2501.05452/x7.png)

> üîº This figure illustrates the process of creating a training dataset for multimodal large language models using the ReFocus framework and the ChartQA dataset.  It shows how ReFocus guides the model to perform visual edits on chart images, generating a chain of thought and intermediate visual artifacts. These visual edits, along with the original question, the model's reasoning, and the final answer, constitute a single data point in the new training dataset. The overall process enhances the model's learning by providing intermediate reasoning steps in visual form, leading to improved performance.
> <details>
> <summary>read the caption</summary>
> Figure 6: Training set collection using ReFocus on ChartQA dataset.
> </details>



![](https://arxiv.org/html/2501.05452/x8.png)

> üîº This figure presents a bar chart visualizing the frequency of visual editing actions performed by the REFOCUS framework across various datasets.  It shows, for each dataset, the percentage of images where the underlying multimodal LLM (GPT-4 in this case) decided to generate and execute code to perform visual edits (such as drawing boxes, masking regions, or highlighting areas). The datasets include: VWTQ, VWTQ_syn, VTabFact, CharXiv, Horizontal Bar, and Vertical Bar. The chart offers a quantitative insight into the extent to which the model leveraged visual edits within its reasoning process for different types of structured images.
> <details>
> <summary>read the caption</summary>
> Figure 7: Statistics of how often visual editing are performed.
> </details>



![](https://arxiv.org/html/2501.05452/x13.png)

> üîº This figure demonstrates the visual chain of thought reasoning process of two different methods: The Phi-3.5-vision model fine-tuned with ReFocus data and the ReFocus + GPT-4o prompting method.  Both methods highlight relevant regions of interest in chart images to facilitate visual reasoning. The fine-tuned model directly outputs the areas to focus on in text format with bounding box coordinates, whereas ReFocus + GPT-4o uses an iterative process, generating visual edits and feeding them back into the model until a final answer is reached. The red boxes in the images highlight the regions identified by each method as crucial for answering the questions. By comparing the highlighted areas, one can observe the similarities and differences in how each method identifies salient features for visual reasoning. 
> <details>
> <summary>read the caption</summary>
> Figure 8: Phi-3.5-vision finetuned with ReFocus visual chain of thought data outputs the areas to focus on. For illustration purposes, we draw these areas in red boxes, and compare with the ReFocus + GPT-4o prompting output.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
| Model | VWTQ | VWTQ_syn | VTabFact | CharXiv | Horizontal Bar | Vertical Bar |
|---|---|---|---|---|---|---|
| Multimodal LLMs with original visual inputs |
| LLaVA-NeXT-7B [20] | 21.7 | 24.0 | 56.8 | 16.1 | 8.1 | 7.3 |
| LLaVA-NeXT-13B [20] | 25.6 | 30.4 | 62.4 | 18.9 | 13.5 | 13.1 |
| LLaVA-NeXT-34B [20] | 36.4 | 38.0 | 71.2 | 18.9 | 23.4 | 12.6 |
| Phi 3 vision [1] | 44.7 | 53.2 | **74.4** | 16.2 | **60.8** | **66.5** |
| Multimodal LLMs with ReFocus edited visual inputs |
| LLaVA-NeXT-7B + Oracle ReFocus | 25.7 | 26.8 | 55.2 | 15.4 | 6.8 | 8.4 |
| LLaVA-NeXT-13B + Oracle ReFocus | 30.3 | 31.6 | 61.2 | 17.5 | 15.5 | 13.1 |
| LLaVA-NeXT-34B + Oracle ReFocus | 39.1 | 41.2 | 68.0 | **21.0** | 26.1 | 15.2 |
| Phi 3 vision + Oracle ReFocus | **48.3** | **56.4** | 72.8 | 17.6 | 60.4 | **66.5** |{{< /table-caption >}}
> üîº This table compares the performance of several open-source multimodal large language models (LLMs) on structured image understanding tasks.  It contrasts their accuracy when processing original images versus when using images that have been edited by the ReFocus method.  Specifically, the '+ oracle ReFocus' results show the LLM's performance using only the final, edited image produced by ReFocus, demonstrating the impact of ReFocus's visual editing on the model's ability to understand structured images.
> <details>
> <summary>read the caption</summary>
> Table 2: Open-source models‚Äô performance upon the original visual input versus GPT-4o + ReFocus edited images as visual input (referred by + oracle ReFocus). Notice that + oracle ReFocus uses the visual artifact generated in the last action of GPT-4o + ReFocus as inputs.
> </details>

{{< table-caption >}}
| Model | VWTQ | VWTQ_syn | VTabFact | CharXiv | Horizontal Bar | Vertical Bar |
|---|---|---|---|---|---|---|
| **Table** |  |  |  |  |  |  |
| GPT-4o Model |  |  |  |  |  |  |
| Text input | 68.1 | 69.6 | 80.0 | \ | 66.2 | 74.9 |
| Figure Input | 61.0 | 68.4 | 88.4 | 47.9 | 75.2 | 74.9 |
| Text + Figure Input | 67.5 | 72.8 | 91.6 | \ | 75.7 | 81.7 |
| GPT-4o + ReFocus |  |  |  |  |  |  |
| Figure Input | 77.2 | 82.8 | 90.8 | 57.3 | 85.4 | 81.2 |
|  | +16.2 | +14.4 | +2.4 | +10.6 | +9.7 | +0.5 |{{< /table-caption >}}
> üîº This table compares the performance of GPT-4o with and without ReFocus on various structured image understanding tasks.  The 'gold text input' refers to providing the model with the text version of the tables instead of the images. The key finding is that ReFocus improves GPT-4o's performance to a level comparable to using the text data directly. The table highlights the gains from visual editing, especially considering that Table 1 shows conversational performance without visual edits, and this table focuses on direct question answering, leading to slight discrepancies in scores.
> <details>
> <summary>read the caption</summary>
> Table 3: ReFocus empowers GPT-4o to achieve the performance as if given gold text input. The text input are mainly csv tables, as detailed in Section¬†3.1. Notice that Table¬†1 reports the conversational performance without visual editing, whereas the performance discussed here is based on direct question answering, leading to minor differences.
> </details>

{{< table-caption >}}
| Dataset | Original | Mask out | Draw Box | Highlight |
|---|---|---|---|---|
| VWTQ | 66.4 | 77.2 | **77.6** | 74.8 |
| VWTQ_syn | 70.4 | **82.4** | 78.8 | 80.8 |{{< /table-caption >}}
> üîº This table presents the results of an ablation study to analyze how different visual editing tools used in the REFOCUS framework impact model performance.  The study controls for the type of visual editing tool (mask out, draw box, highlight) while keeping the overall methodology consistent.  The evaluation metrics are applied to two datasets, VWTQ and VWTQ_syn,  allowing for comparison of the effectiveness across different data and editing techniques.
> <details>
> <summary>read the caption</summary>
> Table 4: Analysis on how different editing tool can affect model performance. We control the tool type provided by ReFocus and experiment on the VWTQ and VWTQ_syn datasets.
> </details>

{{< table-caption >}}
|           | Horizontal Bar | Vertical Bar | Avg. |
|---|---|---|---|
| *QA Prompting* |           |           |           |
| Phi-3.5-vision [12] | 60.1 | 63.1 | 61.5 |
| SFT w/ QA Data | 60.1 | 65.5 | 62.6 |
| *Visual CoT Prompting* |           |           |           |
| Phi-3.5-vision [12] | 69.4 | 66.8 | 68.2 |
| SFT w/ QA Data | 60.6 | 66.8 | 63.4 |
| SFT w/ ReFocus CoT | 67.1 | 70.7 | 68.8 |
| SFT w/ ReFocus VCoT | **71.0** | **72.2** | **71.4** |{{< /table-caption >}}
> üîº This table presents the results of supervised fine-tuning (SFT) experiments on the Phi-3.5-vision model.  The model was fine-tuned using three different datasets: standard Question Answering (QA) pairs, Chain-of-Thought (CoT) data, and ReFocus Visual Chain-of-Thought (VCoT) data. The ReFocus VCoT data includes bounding box coordinates specifying the areas of focus within the images, which is the key difference from the other two datasets. The table shows the accuracy achieved on Horizontal and Vertical Bar chart tasks for each dataset and prompting method (QA prompting and Visual CoT prompting).  The results highlight the impact of incorporating visual reasoning information (bounding boxes) into the training data on model performance.
> <details>
> <summary>read the caption</summary>
> Table 5: SFT accuracy results. The difference between ReFocus VCoT data and CoT data is that VCoT contains refocus area bounding box coordinates whereas CoT does not. All trainings select the best performing hyper-parameters on the same set of training data.
> </details>

{{< table-caption >}}
|               | Horizontal Bar | Vertical Bar | Total |
| :------------ | :---------------: | :------------: | :----: |
| QA Data [24] |      4,990      |     10,069     | 15,059 |
| ReFocus Data  |      4,722      |      9,622     | 14,344 |
| w/ Editing   |      4,220      |      8,599     | 12,819 |{{< /table-caption >}}
> üîº This table presents a statistical summary of the dataset used for training in the REFOCUS model. It shows the total number of training samples in the ChartQA dataset, the number of samples that involved visual editing, and the breakdown of these numbers across different chart types (horizontal bar, vertical bar, etc.). This information provides insights into the composition and characteristics of the dataset used to train the model.
> <details>
> <summary>read the caption</summary>
> Table 6: Detailed statistics about ReFocus Data. We count the total number of our training cases, and the ones with visual editing.
> </details>

{{< table-caption >}}
| Header | w/ default QA Data | w/ ReFocus VCoT | w/ ReFocus CoT |
|---|---|---|---|
| data type | bf16 | bf16 | bf16 |
| batch size | 64 | 64 | 64 |
| learning rate | 5e-07 | 1e-06 | 5e-06 |
| epoch number | 2 | 2 | 2 |
| include edited image in input | No | No | Yes |{{< /table-caption >}}
> üîº This table presents the hyperparameter settings used to achieve the best performance in the fine-tuning experiments of the REFOCUS model.  It lists the data type, batch size, learning rate, number of epochs, and whether or not edited images were included as input for three different fine-tuning scenarios: with default QA data, with REFOCUS VCoT (visual chain-of-thought) data, and with REFOCUS CoT data.
> <details>
> <summary>read the caption</summary>
> Table 7: Hyper-parameter settings for our best fine-tuned models.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="paper_images/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="paper_images/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}