[{"figure_path": "https://arxiv.org/html/2501.05452/x2.png", "caption": "Figure 1: Overview of ReFocus. ReFocus performs visual chain of thought via input-image editing on an example data from TableVQA\u00a0[16]. Given an image and question pair, ReFocus equips GPT-4 with editing tools (details in \u00a73), and GPT-4 generates pseudo code if an edit action is needed. ReFocus then executes the editing actions, and feeds GPT-4 with the new image until an answer is reached. In the above example, mask_column and draw_row are performed.", "description": "This figure illustrates the ReFocus framework. ReFocus enhances large language models (LLMs) by incorporating visual reasoning steps.  It uses Python code to perform image editing, refining attention to key visual elements.  The process is iterative; the LLM generates code for a visual editing operation, the editing is performed on the image, and the modified image is fed back into the LLM. This continues until an answer is produced. The example shown depicts a TableVQA task where irrelevant columns are masked, and relevant rows are highlighted with a bounding box.", "section": "3. REFOCUS"}, {"figure_path": "https://arxiv.org/html/2501.05452/x3.png", "caption": "Figure 2: Example of how ReFocus + GPT-4o solves previously unsolvable problem in ChartQA dataset\u00a0[24] through improved visual grounding. Given the original horizontal bar image (left), GPT-4o grounds to the wrong bars and thus gets the wrong answer. ReFocus eliminates such possibility through editing, guiding the model to the correct answer (right).", "description": "This figure demonstrates how ReFocus improves visual grounding in a chart question answering (ChartQA) task. The left panel shows an original horizontal bar chart, where GPT-4 incorrectly identifies the bars to answer the question. ReFocus modifies the image by visually editing it. This allows GPT-4 to correctly interpret the chart and produce the correct answer (right panel). The improvement highlights how ReFocus enhances the reasoning process.", "section": "3. REFOCUS"}, {"figure_path": "https://arxiv.org/html/2501.05452/x4.png", "caption": "Figure 3: ReFocus equips GPT-4 with selective attention. Above is an example of how ReFocus + GPT-4o solves previously unsolvable problem in ChartXiv dataset\u00a0[34]. Specifically, ReFocus edits upon the original image by masking out all irrelevant information \u2013 the other three subplots that could be distracting. As a result, GPT-4o is able to conduct better reasoning with the edited image, and reach the correct answer.", "description": "This figure demonstrates how REFOCUS, a framework that incorporates visual editing into the reasoning process of large language models (LLMs), improves the ability of GPT-4 to solve complex problems. The example shown involves the ChartXiv dataset, specifically a chart with four subplots. REFOCUS edits the original image by masking out three irrelevant subplots, thereby enabling GPT-4 to focus solely on the relevant subplot and reach the correct answer, overcoming limitations of standard LLMs that struggle with such multi-hop visual reasoning problems.", "section": "3. REFOCUS"}, {"figure_path": "https://arxiv.org/html/2501.05452/x5.png", "caption": "Figure 4: ReFocus unleashes better visual grounding and counting abilities for GPT-4 as in ChartQA\u00a0[24] Vertical Bar problems.", "description": "Figure 4 demonstrates how REFOCUS improves GPT-4's performance on a visual question answering task involving vertical bar charts from the ChartQA dataset.  The left panel shows GPT-4's original response to the question \"How many years have value less than 10%\" applied to a chart displaying year-over-year percentage changes. GPT-4 incorrectly identifies four years. The right panel presents the improved response after employing REFOCUS. REFOCUS guides GPT-4 to focus on the relevant parts of the chart, leading to the correct answer of five years.", "section": "3.2 Visual Editing Tools"}, {"figure_path": "https://arxiv.org/html/2501.05452/x6.png", "caption": "Figure 5: ReFocus unleashes better OCR for GPT-4. In this example from TableVQA\u00a0[16], ReFocus + GPT-4 conducts the edit action highlight_column. With this simple action, GPT-4 can focus more on the important subarea, and recognize the characters better.", "description": "The figure showcases how REFOCUS enhances the performance of GPT-4 in Optical Character Recognition (OCR) tasks within the context of structured image understanding.  Specifically, it demonstrates the use of the 'highlight_column' visual editing function within the REFOCUS framework. By highlighting a relevant column in a table image, REFOCUS guides GPT-4 to focus its attention on the crucial region, thereby improving the accuracy of character recognition and reducing errors.  This example is taken from the TableVQA dataset.", "section": "3.2 Visual Editing Tools"}, {"figure_path": "https://arxiv.org/html/2501.05452/x7.png", "caption": "Figure 6: Training set collection using ReFocus on ChartQA dataset.", "description": "This figure illustrates the process of creating a training dataset for multimodal large language models using the ReFocus framework and the ChartQA dataset.  It shows how ReFocus guides the model to perform visual edits on chart images, generating a chain of thought and intermediate visual artifacts. These visual edits, along with the original question, the model's reasoning, and the final answer, constitute a single data point in the new training dataset. The overall process enhances the model's learning by providing intermediate reasoning steps in visual form, leading to improved performance.", "section": "Finetune with REFOCUS data"}, {"figure_path": "https://arxiv.org/html/2501.05452/x8.png", "caption": "Figure 7: Statistics of how often visual editing are performed.", "description": "This figure presents a bar chart visualizing the frequency of visual editing actions performed by the REFOCUS framework across various datasets.  It shows, for each dataset, the percentage of images where the underlying multimodal LLM (GPT-4 in this case) decided to generate and execute code to perform visual edits (such as drawing boxes, masking regions, or highlighting areas). The datasets include: VWTQ, VWTQ_syn, VTabFact, CharXiv, Horizontal Bar, and Vertical Bar. The chart offers a quantitative insight into the extent to which the model leveraged visual edits within its reasoning process for different types of structured images.", "section": "4. Experiments and Analyses"}, {"figure_path": "https://arxiv.org/html/2501.05452/x13.png", "caption": "Figure 8: Phi-3.5-vision finetuned with ReFocus visual chain of thought data outputs the areas to focus on. For illustration purposes, we draw these areas in red boxes, and compare with the ReFocus + GPT-4o prompting output.", "description": "This figure demonstrates the visual chain of thought reasoning process of two different methods: The Phi-3.5-vision model fine-tuned with ReFocus data and the ReFocus + GPT-4o prompting method.  Both methods highlight relevant regions of interest in chart images to facilitate visual reasoning. The fine-tuned model directly outputs the areas to focus on in text format with bounding box coordinates, whereas ReFocus + GPT-4o uses an iterative process, generating visual edits and feeding them back into the model until a final answer is reached. The red boxes in the images highlight the regions identified by each method as crucial for answering the questions. By comparing the highlighted areas, one can observe the similarities and differences in how each method identifies salient features for visual reasoning. ", "section": "4. Experiments and Analyses"}]