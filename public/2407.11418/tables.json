[{"content": "| Operator | Description |\n|---|---| \n| \\mathit{sem_filter}(l\\textit{: }X\\rightarrow\\mathit{Bool}) | Returns the tuples in a table that pass the provided langex predicate. |\n| \\mathit{sem_join}(t\\textit{: }T{\\textit{,   }{l}}\\textit{: }(X,Y)\\rightarrow \\mathit{Bool}) | Joins a table against a second table  $t$ by keeping all pairs of tuples that pass the provided langex predicate. |\n| \\mathit{sem_agg}(l\\textit{: }L[X]\\rightarrow\\mathit{X}) | Performs an aggregation over the input tuples according to the langex, which specifies a commutative, associative aggregation function over a list of tuples. |\n| \\mathit{sem_topk}(l\\textit{: }L[X]\\rightarrow L[X]{\\textit{,   }{k}}\\textit{: }int) | Ranks each tuple and returns the $k$ best according to the langex, which specifies a ranking function that sorts a list of tuples. |\n| \\mathit{sem_group_by}(l\\textit{: }X\\rightarrow Y{\\textit{,   }{C}}\\textit{: }int) | Groups the tuples into $C$ categories based on the langex, which specifies a grouping criteria. |\n| \\mathit{sem_map}(l\\textit{: }X\\rightarrow Y) | Performs a projection, returning a new column, according to the provided langex. |", "caption": "Table 1. Summary of Key Semantic Operators. T\ud835\udc47Titalic_T denotes a relation, X\ud835\udc4bXitalic_X and Y\ud835\udc4cYitalic_Y denote arbitrary tuple types, L\u2062[X]\ud835\udc3fdelimited-[]\ud835\udc4bL[X]italic_L [ italic_X ] denotes a list of elements with type X\ud835\udc4bXitalic_X, and A\ud835\udc34Aitalic_A denotes the type of a particular column or attribute. l\ud835\udc59litalic_l denotes a parameterized natural language expression (\u201clangex\u201d for short), which takes tuples as input and performs a function such as a predicate, an aggregation, a comparator, or a projection, depending on the operator\u2019s signature.", "description": "This table summarizes the key semantic operators used in the LOTUS system.  It details the function of each operator, including the input types (relation T, tuple types X and Y, list of elements L[X], and attribute type A), and the parameterized natural language expression (langex, l) used to define the operation. The langex serves as a flexible way to specify predicates, aggregations, comparisons, or projections, depending on the operator. The table clarifies how these operators extend the relational model with AI-based capabilities for querying text data.", "section": "2 SEMANTIC OPERATORS"}, {"content": "| Method | Accuracy | Execution Time (s), batched | Execution Time (s), no batching | LoC |\n|---|---|---|---|---|\n| FacTool | 80.9 | N/A | 5396.11 | ~ 750 |\n| LOTUS-Factool | 89.9 | 688.90 | 4,454.24 | ~ 50 |\n| LOTUS-fact-filter | 91.2 | 329.1 | 988.95 | ~ 50 |\n| LOTUS-fact-filter (opt.)* | 91.0 | 189.88 | 776.37 | ~ 50 |\n| LOTUS-fact-join | 84.5 | 11,951.35 | 60,364.39 | ~ 50 |", "caption": "Table 2. Fact-checking Results on the Fever Dataset.", "description": "This table presents the results of fact-checking experiments conducted on the FEVER dataset.  It compares the accuracy, execution time (both with and without batching), and lines of code of different methods: FacTool (a state-of-the-art baseline), and three LOTUS programs with varying implementations (re-implementation of FacTool, filter-based, and join-based approaches). The table highlights the performance and efficiency of LOTUS in solving the fact-checking task, demonstrating that LOTUS requires significantly fewer lines of code while achieving comparable or even improved accuracy with shorter execution times compared to the baseline.", "section": "5 EVALUATION"}, {"content": "| Method | RP@5 | RP@10 | Execution Time (s) | # LM Calls |\n|---|---|---|---|---|\n| Sem-sim-join | 0.106 | 0.120 | 2.91 | 0.00 |\n| LOTUS Sem-join** | 0.244 | 0.261 | 5,891.6 | 15,107 |\n| nested-loop pattern | N/A | N/A | 2,144,560* | 6,092,500 |\n| map-sim-filter pattern** | 0.244 | 0.261 | 5,891.6 | 15,107 |\n| sim-filter pattern** | 0.154 | 0.191 | 24,206.8 | 63,724 |", "caption": "Table 3. Extreme Multi-label Classification Results on Biodex Dataset with Llama-70b", "description": "This table presents the results of extreme multi-label classification experiments conducted on the Biodex dataset using the Llama-70b language model.  It compares the performance of several methods, including a simple semantic similarity join and different variants of the LOTUS semantic join. The metrics used for evaluation include accuracy, rank-precision@5 (RP@5), and rank-precision@10 (RP@10).  Execution time and the number of large language model calls are also reported, offering insight into computational costs and efficiency.", "section": "5.2 Application: Extreme Multi-label Classification"}, {"content": "| Method | nDCG@10 | ET (s) |\n|---|---|---|\n| Search | 0.712 | 0.009 |\n| Search + Reranker | 0.741 | 2.64 |\n| LOTUS Top-k - Llama-70B | 0.775 | 33.6 |\n| LOTUS Top-k - GPT-4o | 0.800 | 11.2 |", "caption": "Table 4. Ranking Results on SciFact", "description": "This table presents the results of the SciFact ranking task, comparing different methods.  It shows the nDCG@10 (normalized Discounted Cumulative Gain at rank 10) scores, a metric measuring ranking quality, along with the execution time (ET) for each approach.  The methods compared include simple semantic search, search with a reranker, and the LOTUS system using two different language models (Llama-70B and GPT-40).  Higher nDCG@10 indicates better ranking performance, while lower ET suggests faster query execution.", "section": "5.3 Application: Search & Ranking"}, {"content": "| Method | CIFAR nDCG@10 | CIFAR ET (s) | HellaSwag nDCG@10 | HellaSwag ET (s) |\n|---|---|---|---|---|\n| Search | 0.252 | 0.008 | 0.119 | 0.008 |\n| Search + Reranker | 0.001 | 2.57 | 0.461 | 2.36 |\n| LOTUS Top-k - Llama 70B | 0.710 | 39.6 | 0.975 | 63.6 |", "caption": "Table 5. Ranking Results on CIFAR- & HellaSwag-bench", "description": "This table presents the results of a ranking task performed on two newly created benchmarks: CIFAR-bench and HellaSwag-bench.  These benchmarks evaluate the ability of different methods to rank scientific papers based on their reported accuracy for the CIFAR-10 and HellaSwag datasets. The table compares the performance of three methods: a semantic search, a semantic search with a re-ranker, and the LOTUS system's semantic top-k approach using the Llama 70B language model. For each method, the table shows the nDCG@10 score (a measure of ranking quality) and the execution time in seconds.", "section": "5.3 Application: Search & Ranking"}, {"content": "| Method | Scifact nDCG@10 | Scifact ET (s) | Scifact # LM Calls | CIFAR nDCG@10 | CIFAR ET (s) | CIFAR # LM Calls | HellaSwag nDCG@10 | HellaSwag ET (s) | HellaSwag # LM Calls |\n|---|---|---|---|---|---|---|---|---|---| \n| Quadratic Sort | 0.836 | 712 | 4950 | 0.868 | 634 | 4950 | 0.966 | 1,803 | 19,900 |\n| Heap Top-k | 0.776 | 65.0 | 216 | 0.832 | 99.6 | 350 | 0.907 | 98.9 | 415.2 |\n| QuickSelect Top-k | 0.776 | 42.4 | 285 | 0.746 | 41.3 | 303.95 | 0.909 | 59.1 | 620.95 |\n| QuickSelect Top-k + Semantic Index | 0.775 | 33.6 | 229 | 0.710 | 39.6 | 307.7 | 0.975 | 63.6 | 672.7 |", "caption": "Table 6. Comparison of Ranking Results for Different Semantic Top-k Algorithms using Llama-70B", "description": "This table compares the performance of four different algorithms used for semantic top-k ranking, all using the Llama-70B language model.  It shows the nDCG@10 score (a measure of ranking quality), execution time in seconds, and the number of Language Model calls required for each algorithm across three different datasets: SciFact, CIFAR-bench, and HellaSwag-bench. The algorithms compared are Quadratic Sort, Heap Top-k, QuickSelect Top-k, and a QuickSelect Top-k algorithm optimized with a semantic index. The datasets vary in complexity, with SciFact focusing on relevance ranking and the other two using more complex ranking criteria based on reported accuracy. This allows for a comprehensive comparison of the algorithms under various conditions.", "section": "4.4 sem_topk"}, {"content": "| \u03bc | Description |\n|---|---| \n| \u03bc\u2081 | Advancements in Recommender Systems and Multimodal Data Integration |\n| \u03bc\u2082 | Advancements in Generative Information Retrieval Systems |\n| \u03bc\u2083 | Advancements in Large Language Models for Various Applications |\n| \u03bc\u2084 | Advancements in AI Security and Malware Detection Techniques |\n| \u03bc\u2085 | Advancements in Robotic Navigation and Manipulation Techniques |", "caption": "Table 7. Discovered Group Labels Over ArXiv Papers", "description": "This table presents the five group labels automatically discovered by the LOTUS system when applied to a dataset of 647 arXiv papers.  Each label summarizes a common theme or topic found within a cluster of papers. The goal of this unsupervised grouping task is to demonstrate the capabilities of the semantic group-by operator in LOTUS and the quality of its automatic label generation.  The effectiveness of these automatically generated labels is further evaluated in a later section of the paper by comparing the accuracy of a classifier that uses these labels against other baselines.", "section": "5.4 Application: ArXiv Paper Analysis"}]