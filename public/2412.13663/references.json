{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-04", "reason": "This paper introduced the Transformer architecture, a fundamental building block of ModernBERT and many other modern language models."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2019-06-02", "reason": "BERT was a groundbreaking model that significantly advanced the field of NLP, providing the foundation for ModernBERT and its improvements."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language Models are Unsupervised Multitask Learners", "publication_date": "2019-05-01", "reason": "This paper highlighted the effectiveness of large language models on various NLP tasks, influencing the approach of ModernBERT's pretraining and downstream task evaluation."}, {"fullname_first_author": "Omar Khattab", "paper_title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "publication_date": "2020-07-25", "reason": "ColBERT introduced a highly efficient retrieval method used in ModernBERT's evaluation, showcasing the importance of retrieval in many NLP applications."}, {"fullname_first_author": "Vladimir Karpukhin", "paper_title": "Dense Passage Retrieval for Open-Domain Question Answering", "publication_date": "2020-11-16", "reason": "DPR, introduced in this paper, is a key technique for efficient information retrieval used in ModernBERT's evaluation, demonstrating the continued importance of effective retrieval methods."}]}