[{"content": "|                     | Model     | IR (DPR) BEIR | IR (DPR) MLDR<sub>OOD</sub> | IR (DPR) MLDR<sub>ID</sub> | IR (ColBERT) BEIR | IR (ColBERT) MLDR<sub>OOD</sub> | NLU    | Code GLUE | Code CSN | Code SQA |\n| :------------------ | :-------- | :---------------: | :-----------------------: | :-----------------------: | :----------------: | :---------------------------: | :-----: | :--------: | :------: | :------: |\n| <div style=\"width:8.5pt;\"><div style=\"width:6.9pt;height:88.7pt;vertical-align:-40.9pt;\"><div style=\"width:85.4pt;\">Base</div></div></div> | BERT      | 38.9             | 23.9                     | 32.2                     | 49.0             | 28.1                         | 84.7   | 41.2      | 59.5     |        |\n|                     | RoBERTa   | 37.7             | 22.9                     | 32.8                     | 48.7             | 28.2                         | 86.4   | 44.3      | 59.6     |        |\n|                     | DeBERTaV3 | 20.2             | 5.4                      | 13.4                     | 47.1             | 21.9                         | 88.1   | 17.5      | 18.6     |        |\n|                     | NomicBERT | 41.0             | 26.7                     | 30.3                     | 49.9             | 61.3                         | 84.0   | 41.6      | 61.4     |        |\n|                     | GTE-en-MLM | 41.4             | **34.3**                 | 44.4                     | 48.2             | 69.3                         | 85.6   | 44.9      | 71.4     |        |\n|                     | ModernBERT | **41.6**         | 27.4                     | **44.0**                 | **80.2**         | **88.4**                     | **56.4** | **73.6** |          |        |\n| <div style=\"width:8.5pt;\"><div style=\"width:8.8pt;height:60.2pt;vertical-align:-26.6pt;\"><div style=\"width:56.9pt;\">Large</div></div></div> | BERT      | 38.9             | 23.3                     | 31.7                     | 49.5             | 28.5                         | 85.2   | 41.6      | 60.8     |        |\n|                     | RoBERTa   | 41.4             | 22.6                     | 36.1                     | 49.8             | 28.8                         | 88.9   | 47.3      | 68.1     |        |\n|                     | DeBERTaV3 | 25.6             | 7.1                      | 19.2                     | 46.7             | 23.0                         | **91.4** | 21.2      | 19.7     |        |\n|                     | GTE-en-MLM | 42.5             | **36.4**                 | **48.9**                 | 50.7             | 71.3                         | 87.6   | 40.5      | 66.9     |        |\n|                     | ModernBERT | **44.0**         | 34.3                     | **52.4**                 | **80.4**         | 90.4                         |          | **59.5** | **83.9** |        |", "caption": "Table 1: Results for all models across an overview of all tasks. CSN refers to CodeSearchNet and SQA to StackQA. MLDRID refers to in-domain (fine-tuned on the training set) evaluation, and MLDROOD to out-of-domain.", "description": "This table presents the performance of various encoder-only models across multiple downstream tasks.  It compares the performance of ModernBERT (the model introduced in the paper) against several established models (BERT, RoBERTa, DeBERTa-v3, NomicBERT, and GTE-en-MLM). The tasks encompass a range of natural language understanding benchmarks (GLUE), code retrieval tasks (CodeSearchNet, StackQA), and text retrieval tasks (BEIR, MLDR).  For text retrieval, the table distinguishes between in-domain (MLDRID, fine-tuned on training data) and out-of-domain (MLDROOD) evaluations.  This allows for a comprehensive comparison of model performance across diverse tasks and data settings.", "section": "3 Downstream Evaluation"}, {"content": "| MLDR<sub>OOD</sub> |\n", "caption": "Table 2: Memory (max batch size, BS) and Inference (in thousands of tokens per second) efficiency results on an NVIDIA RTX 4090, averaged over 10 runs. Dashes indicate unsupported configurations.", "description": "This table presents a comprehensive comparison of the memory usage and inference speed of various encoder-only models.  The results were obtained using an NVIDIA RTX 4090 GPU, averaging over 10 runs for each model.  Memory efficiency is represented by the maximum batch size (BS) that can be processed. Inference speed is measured in thousands of tokens processed per second.  The table also indicates where configurations were not supported.", "section": "4 Efficiency"}, {"content": "| MLDR<sub>ID</sub>|\n|---|---|", "caption": "Table 3: ModernBERT training settings. Dropout and below are shared across all phases.", "description": "This table details the hyperparameters and settings used during the training of the ModernBERT models.  It breaks down the training process into phases (pretraining, context extension phase one and two) for both the base and large versions of the model.  For each phase and model size, the table specifies training tokens, maximum sequence length, batch size, warmup tokens, learning rate, schedule type, total and training time, model initialization method, and dropout rates for different model layers.", "section": "2 Methods"}, {"content": "| GLUE |\n|---|---|", "caption": "Table 4: ModernBERT model design", "description": "This table details the specific architectural design choices made for the two ModernBERT models: base and large.  It covers key hyperparameters and design decisions impacting performance and efficiency, such as the number of layers, hidden size, activation functions (GeLU), attention mechanisms (global and local), normalization type (LayerNorm), and the use of rotary positional embeddings (RoPE).  It also specifies the presence or absence of bias terms and other implementation details crucial to understanding the model's construction and the reasoning behind those choices.", "section": "2 Methods"}, {"content": "| CSN |\n|---|---|", "caption": "Table 5: GLUE\u00a0Wang et\u00a0al. (2018) dev set scores. \u03b1 taken from Table 8 of\u00a0Liu et\u00a0al. (2019a), \u03b2 taken from Table S3 of\u00a0Portes et\u00a0al. (2023), \u03b3 from Table 2 of\u00a0Nussbaum et\u00a0al. (2024), \u03b4 from Table 21 of\u00a0Zhang et\u00a0al. (2024), \u03f5 from Table 2 of\u00a0Qiang et\u00a0al. (2024) and \u03b6 from Table 3 of\u00a0He et\u00a0al. (2023)", "description": "This table presents the results of several encoder-only models on the GLUE benchmark, a widely used evaluation suite for natural language understanding tasks.  The results are broken down by model size (base and large), showing performance on various subtasks within the GLUE benchmark such as single sentence tasks (COLA, SST-2), paraphrase and similarity tasks (MRPC, STS-B, QQP), and natural language inference tasks (MNLI, QNLI, RTE).  Note that results for some models (indicated by Greek letters) were obtained from other papers and included for comparison.", "section": "3.1 Evaluation Setting"}, {"content": "| SQA |\n|---|---|", "caption": "Table 6: Fine-tuning hyperparameters for ModernBERT on GLUE tasks. LR: Learning Rate, WD: Weight Decay, Ep: Epochs.", "description": "This table shows the hyperparameters used for fine-tuning the ModernBERT model on the GLUE benchmark.  It lists the learning rate (LR), weight decay (WD), and number of epochs (Ep) used for each of the GLUE tasks.  The values were determined through hyperparameter searches to optimize performance on each specific task within the GLUE benchmark.", "section": "2.2 Training"}, {"content": "|       |       | Params      | BS           | Fixed    | Variable | BS           | Fixed    | Variable |\n| :---- | :---- | :---------- | :----------- | :-------- | :-------- | :----------- | :-------- | :-------- |\n|       |       |             | Short        |           |           | Long        |           |           |\n|       | Model |             |              |           |           |              |           |           |\n| Base  | BERT  | 110M        | 1096         | **180.4** | 90.2      | \u2013           | \u2013         | \u2013         |\n|       | RoBERTa | 125M        | 664          | 179.9     | 89.9      | \u2013           | \u2013         | \u2013         |\n|       | DeBERTaV3 | 183M        | 236          | 70.2      | 35.1      | \u2013           | \u2013         | \u2013         |\n|       | NomicBERT | 137M        | 588          | 117.1     | 58.5      | 36          | 46.1      | 23.1      |\n|       | GTE-en-MLM | 137M        | 640          | 123.7     | 61.8      | 38          | 46.8      | 23.4      |\n|       | GTE-en-MLMxformers | 137M        | 640          | 122.5     | 128.6     | 38          | 47.5      | 67.3      |\n|       | ModernBERT | 149M        | **1604**     | 148.1     | **147.3** | **98**      | **123.7** | **133.8** |\n| Large | BERT  | 330M        | **792**      | **54.4**  | 27.2      | \u2013           | \u2013         | \u2013         |\n|       | RoBERTa | 355M        | 460          | 42.0      | 21.0      | \u2013           | \u2013         | \u2013         |\n|       | DeBERTaV3 | 434M        | 134          | 24.6      | 12.3      | \u2013           | \u2013         | \u2013         |\n|       | GTE-en-MLM | 435M        | 472          | 38.7      | 19.3      | 28          | 16.2      | 8.1       |\n|       | GTE-en-MLMxformers | 435M        | 472          | 38.5      | 40.4      | 28          | 16.5      | 22.8      |\n|       | ModernBERT | 395M        | 770          | 52.3      | **52.9**  | **48**      | **46.8**  | **49.8** |", "caption": "Table 7: BEIR\u00a0Thakur et\u00a0al. (2021) nDCG@10 scores for single-vector retrieval models.", "description": "This table presents the performance of various single-vector retrieval models on the BEIR benchmark (Thakur et al., 2021).  The BEIR benchmark is a comprehensive evaluation suite for information retrieval, encompassing a wide variety of tasks and domains. The nDCG@10 metric is used to assess the ranking quality of the retrieved documents for each dataset within BEIR.  Lower nDCG@10 scores suggest poorer retrieval performance.", "section": "3.1.2 Text Retrieval"}, {"content": "|                     | Pretraining Phase | Pretraining Phase | Context Extension: Phase One | Context Extension: Phase One | Context Extension: Phase Two | Context Extension: Phase Two |\n| :------------------ | :----------------- | :----------------- | :-------------------------- | :-------------------------- | :-------------------------- | :-------------------------- |\n|                     | Base               | Large              | Base                         | Large                         | Base                         | Large                         |\n| Training Tokens      | 1.719 trillion      |                     | 250 billion                   |                     | 50 billion                    |                     |\n| Max Sequence Length | 1,024              |                     | 8,192                        |                     | 8,192                        |                     |\n| Batch Size          | 4,608              | 4,928              | 72                           | 77                           | 72                           | 78                           |\n| Warmup (tokens)     | 50 billion          | 10 billion          | -                            | -                            | -                            | -                            |\n| Microbatch Size     | 96                 | 56                 | 12                           | 7                            | 12                           | 6                            |\n| Learning Rate       | 8e-4                | 5e-4, 5e-5          | 3e-4                         | 5e-5                         | 3e-4                         | 5e-5                         |\n| Schedule            | Trapezoidal         |                     | -                            | -                            | 1-sqrt                       |                     |\n| Warmup (tokens)     | 3 billion           | 2 billion           | -                            | -                            | -                            | -                            |\n| Decay (tokens)      | -                  | -                  | -                            | -                            | 50 billion                   |                     |\n| Weight Decay        | 1e-5                | 1e-5, 1e-6          | 1e-5                         | 1e-6                         | 1e-5                         | 1e-6                         |\n| Total Time (hours)  | 194.2               | 425.3               | 39.9                         | 80.7                         | 11.5                         | 21.7                         |\n| Training Time (hours)| 191.1               | 420.4               | 36.3                         | 75.1                         | 7.5                          | 15.3                         |\n| Model Initialization | Megatron            | From Base           | -                            | -                            | -                            | -                            |\n| Dropout (attn out)  | 0.1                 |                     |                     |                     |                     |                     |\n| Dropout (all other layers)| 0.0                 |                     |                     |                     |                     |                     |\n| Optimizer           | StableAdamW          |                     |                     |                     |                     |                     |\n| Betas                | (0.90, 0.98)        |                     |                     |                     |                     |                     |\n| Epsilon              | 1e-06               |                     |                     |                     |                     |                     |\n| Training Hardware   | 8x H100              |                     |                     |                     |                     |                     |\n| Training Strategy   | Distributed DataParallel|                     |                     |                     |                     |                     |\n| Software Libraries  | PyTorch 2.4.0, Cuda 12.4.0, Composer 0.24.1, Flash Attention 2.6.3, FA3 commit 32792d3 |                     |                     |                     |                     |                     |", "caption": "Table 8: BEIR\u00a0Thakur et\u00a0al. (2021) nDCG@10 scores for multi-vector retrieval models.", "description": "This table presents the performance of various encoder models on the BEIR benchmark's multi-vector retrieval task, specifically measuring the normalized discounted cumulative gain (nDCG) at cutoff 10 (nDCG@10).  The BEIR benchmark is a widely used standard evaluation suite for information retrieval tasks, covering diverse domains and queries.  The nDCG@10 metric quantifies the ranking quality of the retrieved documents.  The table shows how effectively each model can retrieve relevant documents given a query using a multi-vector representation technique (where each document is represented by multiple vectors instead of a single vector).", "section": "3.1.2 Text Retrieval"}, {"content": "| Feature | Base | Large |\n|---|---|---|\n| Vocabulary | 50,368 | 50,368 |\n| Unused Tokens | 83 | 83 |\n| Layers | 22 | 28 |\n| Hidden Size | 768 | 1024 |\n| Transformer Block | Pre-Norm | Pre-Norm |\n| Activation Function | GeLU | GeLU |\n| Linear Bias | False | False |\n| Attention | Multi-head | Multi-head |\n| Attention Heads | 12 | 16 |\n| Global Attention | Every three layers | Every three layers |\n| Local Attention Window | 128 | 128 |\n| Intermediate Size | 1,152 | 2,624 |\n| GLU Expansion | 2,304 | 5,248 |\n| Normalization | LayerNorm | LayerNorm |\n| Norm Epsilon | 1e-5 | 1e-5 |\n| Norm Bias | False | False |\n| RoPE theta | 160,000 | 160,000 |\n| Local Attn RoPE theta | 10,000 | 10,000 |", "caption": "Table 9: Learning rate used for reported results on BEIR\u00a0Thakur et\u00a0al. (2021) for both single and multi vector retrieval", "description": "This table shows the optimal learning rates used for fine-tuning different encoder models on the BEIR benchmark.  It breaks down the results by two retrieval methods: single-vector and multi-vector retrieval.  The table helps demonstrate the different hyperparameter settings needed to achieve optimal performance for various models in different retrieval scenarios.", "section": "3.1.2 Text Retrieval"}, {"content": "|                     | Model             | Params | Seq. | CoLA | SST-2 | MRPC | STS-B | QQP  | MNLI | QNLI | RTE |\n| :------------------ | :----------------- | :----- | :---- | :---- | :----- | :---- | :----- | :---- | :---- | :---- | :---- |\n| **Base**           | BERT<sup>\u03b2</sup>     | 110M   | 512   | 59.0  | 93.1   | 89.5  | 89.4   | 91.4  | 85.4  | 91.6  | 78.2 |\n|                     | RoBERTa<sup>\u03b1</sup>   | 125M   | 512   | 63.6  | 94.8   | 90.2  | 91.2   | 91.9  | 87.6  | 92.8  | 78.7 |\n|                     | DeBERTav3<sup>\u03f5</sup> | 183M   | 512   | **69.2** | 95.6   | 89.5  | 91.6   | **92.4** | **90.0** | 83.8  |       |\n|                     | MosaicBERT-128<sup>\u03b2</sup> | 137M   | 128   | 58.2  | 93.5   | 89.0  | 90.3   | 92.0  | 85.6  | 91.4  | 83.0 |\n|                     | NomicBERT-2048<sup>\u03b3</sup> | 137M   | 2048  | 50.0  | 93.0   | 88.0  | 90.0   | 92.0  | 86.0  | 92.0  | 82.0 |\n|                     | GTE-en-MLM<sup>\u03b4</sup> | 137M   | 8192  | 57.0  | 93.4   | 92.1  | 88.8   | 86.7  | 91.9  | 84.8  |       |\n|                     | ModernBERT         | 149M   | 8192  | **96.0** | **92.2** | **91.8** | 92.1   | 89.1  | **93.9** | **87.4** |       |\n| **Large**          | BERT<sup>\u03b2</sup>     | 330M   | 512   | 56.2  | 93.3   | 87.8  | 90.6   | 90.9  | 86.3  | 92.8  | 83.8 |\n|                     | RoBERTa<sup>\u03b1</sup>   | 355M   | 512   | 68.0  | 96.4   | 90.9  | 92.2   | 90.2  | 94.7  | 86.6  |       |\n|                     | DeBERTav3<sup>\u03b6</sup> | 434M   | 512   | **75.3** | 96.9   | 92.2  | **93.3** | **91.8** | **96.0** | **92.7** |       |\n|                     | GTE-en-MLM<sup>\u03b4</sup> | 434M   | 8192  | 60.4  | **93.5** | 91.4  | 89.2   | 89.2  | **93.9** | 88.1  |       |\n|                     | ModernBERT         | 395M   | 8192  | **97.1** | 91.7   | 92.8  | 92.7   | 90.8  | 95.2  | 92.1  |", "caption": "Table 10: Token statistics for the synthetic datasets used in efficiency evaluations.", "description": "This table presents statistics of four synthetic datasets created for evaluating the inference efficiency of different models.  Each dataset contains 8192 documents, but differs in the distribution of token lengths within each document.  The first two datasets, 'Fixed Short' and 'Fixed Long', have uniform token lengths of 512 and 8192 tokens per document, respectively.  The other two, 'Variable Short' and 'Variable Long', feature token lengths that follow a normal distribution centered at 256 and 4096 tokens, respectively, to simulate real-world scenarios where document lengths vary.", "section": "4 Efficiency"}, {"content": "Task|Base LR|Base WD|Base Ep|Large LR|Large WD|Large Ep\n---|---|---|---|---|---|---\nCoLA|8e-5|1e-6|5|3e-5|8e-6|5\nMNLI|5e-5|5e-6|1|3e-5|1e-5|1\nMRPC|5e-5|5e-6|10|8e-5|5e-6|2\nQNLI|8e-5|5e-6|2|3e-5|5e-6|2\nQQP|5e-5|5e-6|10|5e-5|8e-6|2\nRTE|5e-5|1e-5|3|5e-5|8e-6|3\nSST-2|8e-5|1e-5|2|1e-5|1e-6|3\nSTSB|8e-5|5e-6|10|8e-5|1e-5|10", "caption": "Table 11: Inference runtime for all models. Bold indicates the best for the column within two SDs.", "description": "This table presents the inference speed (tokens per second) of different encoder-only models, including ModernBERT, on various sequence lengths.  The results are broken down by model size (base and large), batch size (fixed and variable-length sequences), and sequence length (short and long).  Bold values indicate the fastest inference speed within two standard deviations of the best result for that category.", "section": "4 Efficiency"}]